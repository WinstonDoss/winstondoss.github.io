<!DOCTYPE HTML>
<html>
	<head>
		<title>Human Tracking Project - My Portfolio</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
		<div id="wrapper" class="fade-in">

			<!-- Intro -->
			<div id="intro">
				<h1>Human Tracking Project</h1>
				<ul class="actions">
					<li><a href="#header" class="button icon solid solo fa-arrow-down scrolly">Explore</a></li>
				</ul>
			</div>

			<!-- Header -->
			<header id="header">
				<a href="project.html" class="logo">Back to Projects</a>
			</header>

			<!-- Nav -->
			<nav id="nav">
				<ul class="icons">
					<li><a href="#" class="icon brands fa-linkedin"><span class="label">Linkedin</span></a></li>
					<li><a href="#" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
				</ul>
			</nav>

			<!-- Main Content -->
			<div id="main">
				<!-- Project Overview -->
				<article class="post featured">
					<header class="major">
						<h2>Real-Time Human Tracking with Lidar and Camera Fusion</h2>
					</header>
					<h3>Problem</h3>
					<p>
						Autonomous mobile robots are increasingly deployed in 
						human-centric environments—such as warehouses, 
						hotels, and offices—for applications like material handling, 
						customer interaction, and surveillance. However, effective 
						real-time human tracking in these dynamic settings remains 
						a challenge. Existing solutions often rely on high-cost, 
						sophisticated sensors that increase system complexity and cost,
						limiting large-scale deployment in practical settings. 
						The goal is to develop a cost-effective tracking solution that
						enables robots to reliably detect and track humans in 
						real-time, even in cluttered environments with occlusions.</p>
					<h3>Solution</h3>
					<a class="image main"><img src="images/overall.png" alt="Human Tracking Project" /></a>
					<p>
						This study introduces a low-cost, multi-sensor fusion approach 
						that combines a 2D LiDAR and monocular camera for human tracking
						and position estimation. Key components of the solution 
						include:</p>
					<h4 style="text-align: left;">Detection</h4>
					<p>
						The LiDAR detects human legs within a 4-meter range(after background subtraction), using 
						a combination of density-based clustering and least-squares based circle fitting.
						Clustering is performed to group points which are proximate to each other
						and these clusters are then fit into a circle using least squares.
						If the curvature of the circle is within a set threshold the cluster is considered a 
						detection of a human leg.
						As this is a geometry based method, shapes which are similar to a human leg are falsely
						detected.
						Yolo-v4 based camera detection has been shown to give quite accurate detections of humans but the detections are with respect to the image.
						Therefore we leverage the accuracy of the Yolo model to remove false detections from the lidar detections.
					</p>
					<a class="image main"><img src="images/yolofit.png" alt="" /></a>

					<h4 style="text-align: left;">Data Fusion and Calibration</h4>
					<p>
						The camera and LiDAR data are calibrated and synchronized, 
						with extrinsic and intrinsic calibration aligning the sensors 
						frames. Aligning the frames allows us to project lidar detections
						onto the image frame to remove false positives.
						ROS-based time synchronization ensures data from both sensors
						are processed in real-time, within a 0.6-second threshold, 
						enabling precise temporal alignment.
					</p>
					
					<h4 style="text-align: left;">Extended Kalman Filter (EKF) for Continuous Tracking:</h4>
					<p>
						An EKF processes the combined data to track individuals across
						 frames, using a constant linear and angular velocity motion 
						 model to remove noise and also to predict human trajectories 
						 Predictions are possible even when direct measurements are temporarily unavailable. 
						 This predictive capability allows the system to handle 
						 occlusions and maintain accurate tracking, even in crowded 
						 environments.
					</p>

					<p>For further details, you can read my paper:</p>
					<ul class="actions special">
						<li><a href="https://ieeexplore.ieee.org/document/10745641" class="button large">Read my paper</a></li>
					</ul>				
					
				</article>
			</div>

			<!-- Footer -->
			<footer id="footer">
				<section class="split contact">
					<section class="alt">
						<h3>Contact</h3>
						<p>If you'd like to know more about this project, please reach out.</p>
					</section>
					<section>
						<h3>Email</h3>
						<p><a href="mailto:winston.doss@gmail.com">winston.doss@gmail.com</a></p>
					</section>
				</section>
			</footer>

			<!-- Copyright -->
			<div id="copyright">
				<ul><li>&copy; My Portfolio</li><li>Design: <a href="https://html5up.net">HTML5 UP</a></li></ul>
			</div>

		</div>

		<!-- Scripts -->
		<script src="assets/js/jquery.min.js"></script>
		<script src="assets/js/jquery.scrollex.min.js"></script>
		<script src="assets/js/jquery.scrolly.min.js"></script>
		<script src="assets/js/browser.min.js"></script>
		<script src="assets/js/breakpoints.min.js"></script>
		<script src="assets/js/util.js"></script>
		<script src="assets/js/main.js"></script>

	</body>
</html>
